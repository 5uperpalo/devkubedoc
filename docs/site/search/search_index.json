{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"DEVKUBEDOC [2 min read] \u00b6 Documentation for Kubernetes for development and data science purposes. This repository serves as complementary configuration and documentation for Kubernetes cluster deployed at the CTTC for Sustainable AI Research Unit . Main purpose of this Kubernetes implementation is to deliver an experimental data science programming and testing environment to Sustainable AI Research Unit. An environment where all computational resources could be effectively shared among the Research Unit members and/or internal/external collaborators. An environment that is easy to extend by new servers financed by new grants. An environment that is easy to monitor and administer. Figure 1 Overview of SAI Kubernetes cluster Other advantages include: 1. use of Kubernetes infrastructure for simulation of Federated learning scenarios, e.g EasyFL or KubeFL 2. simulation of IIoT/IoT sensor deployment and analysis of the traffic between deployed pods(sensors), e.g. IoTSimulator , Kubeshark 3. MLOps, monitoring of models performance, ML pipelines etc. by easy helm chart deployment of e.g. MLflow , AirFlow 4. and many more, only sky is the limit... I chose Microk8s Kubernetes implementation as it includes all necessary elements for JupyterHub, it is lightweitgh supports NFS, GPU, NGINX and Ingress out of the box and is easy to administer, e.g see this comparison .","title":"home"},{"location":"index.html#devkubedoc-2-min-read","text":"Documentation for Kubernetes for development and data science purposes. This repository serves as complementary configuration and documentation for Kubernetes cluster deployed at the CTTC for Sustainable AI Research Unit . Main purpose of this Kubernetes implementation is to deliver an experimental data science programming and testing environment to Sustainable AI Research Unit. An environment where all computational resources could be effectively shared among the Research Unit members and/or internal/external collaborators. An environment that is easy to extend by new servers financed by new grants. An environment that is easy to monitor and administer. Figure 1 Overview of SAI Kubernetes cluster Other advantages include: 1. use of Kubernetes infrastructure for simulation of Federated learning scenarios, e.g EasyFL or KubeFL 2. simulation of IIoT/IoT sensor deployment and analysis of the traffic between deployed pods(sensors), e.g. IoTSimulator , Kubeshark 3. MLOps, monitoring of models performance, ML pipelines etc. by easy helm chart deployment of e.g. MLflow , AirFlow 4. and many more, only sky is the limit... I chose Microk8s Kubernetes implementation as it includes all necessary elements for JupyterHub, it is lightweitgh supports NFS, GPU, NGINX and Ingress out of the box and is easy to administer, e.g see this comparison .","title":"DEVKUBEDOC [2 min read]"},{"location":"code_profiling.html","text":"Code profiling [5 min read] \u00b6 JupyterHub(any Kubernetes) pod \u00b6 Max RAM/CPU \u00b6 Using PromQL to query Prometheus metrics of the pod. Max RAM/CPU/GPU from prometheus_api_client import PrometheusConnect from math import ceil def get_pod_max_cpu_memory_in_prometheus( prometheus_url: str = \"http://prometheus-kube-prometheus-prometheus.monitoring:9090\", container_namespace: str = \"jupyterhub\", pod_name: str = \"jupyter-5uperpalo\", ) -> dict: \"\"\" Function to query max CPU and memory consumption of a specific pod in Kubernetes in the past 10 days. container_memory_working_set_bytes metric returns value per spawned pod id (different pods might have been spawned with the same name in the past 10 days) - only the last is checked For the specific query details see the link below: https://stackoverflow.com/questions/58747562/how-to-get-max-cpu-useage-of-a-pod-in-kubernetes-over-a-time-interval-say-30-da https://stackoverflow.com/a/66778814 Args: prometheus_url (str): Prometheus service URL container_namespace (str): Kubernetes pod namespace name pod_name (str): Kubernetes namespace name Returns: metrics (dict): dictionary with values \"\"\" def round_up(n, decimals=0): multiplier = 10**decimals return math.ceil(n * multiplier) / multiplier prom = PrometheusConnect(url=prometheus_url, disable_ssl=True) pod_name = f\"'{pod_name}'\" container_namespace = f\"'{container_namespace}'\" container_cpu_max_query = f\"max_over_time(sum(rate(container_cpu_usage_seconds_total{{namespace={container_namespace}, pod={pod_name}, container!=''}}[1m]))[10d:1m])\" container_memory_max_query = f\"max_over_time(container_memory_working_set_bytes{{namespace={container_namespace}, pod={pod_name}, container!=''}}[10d])\" cpu_max = prom.custom_query(query=container_cpu_max_query) memory_max = prom.custom_query(query=container_memory_max_query) metrics = { \"cpu\": round_up(float(cpu_max[-1][\"value\"][1]), 2), \"memory\": round_up(float(memory_max[-1][\"value\"][1])/1024**3, 2), } return metrics besteffort_pods_list = [ (\"kepler-7dfh2\", \"kepler\"), (\"nginx-ingress-microk8s-controller-h5kfj\", \"ingress\"), ] for pod, namespace in besteffort_pods_list: metrics = get_pod_max_cpu_memory_in_prometheus( container_namespace=namespace, pod_name=pod, ) print(f\"{pod} ({namespace}), {metrics}\") >>> kepler-7dfh2 (kepler), {'cpu': 0.19, 'memory': 0.07} >>> nginx-ingress-microk8s-controller-h5kfj (ingress), {'cpu': 0.12, 'memory': 0.45} Max GPU \u00b6 In case it is necessary a the Prometheus metrics can be extended by including NVIDIA DCGM exporter in exported metrics and then querying the metrics of interest using similar function to get_pod_max_cpu_memory_in_prometheus . Energy measurements \u00b6 Using Kubernetes Efficient Power Level Exporter (Kepler) . Core/Uncore measurements might not be available(only PKG) depending on Intel architecture. See RAPL section in Kepler metrics explanation. Kepler metrics from datetime import datetime from prometheus_api_client import PrometheusConnect def get_kepler_pod_stats( to_timestamp: float, from_timestamp: float, prometheus_url: str = \"http://prometheus-kube-prometheus-prometheus.monitoring:9090\", container_namespace: str = \"jupyterhub\", pod_name: str = \"jupyter-5uperpalo\", ) -> dict: \"\"\"Function to query Kepler power consumption data of specific pod in Kubernetes. # https://sustainable-computing.io/design/kepler-energy-sources/ # https://github.com/sustainable-computing-io/kepler/blob/1c397ff00b72b5cb1585d0de2cd495c73d88f07a/grafana-dashboards/Kepler-Exporter.json#L299 # https://prometheus.io/docs/prometheus/latest/querying/basics/#time-durations # [metric for metric in prom.all_metrics() if \"kepler\" in metric] Args: to_timestamp (list): 'to' timestamp from_timestamp (list): 'from' timestamp prometheus_url (str): Prometheus service url container_namespace (str): Kubernetes pod namespace name pod_name (str): Kubernetes namespace name Returns: metrics (dict): Kepler metrics of the power consumption of pod in Kubernetes \"\"\" prom = PrometheusConnect(url=prometheus_url, disable_ssl=True) pod_name = f\"'{pod_name}'\" container_namespace = f\"'{container_namespace}'\" time_range_sec = str(int(to_timestamp - from_timestamp)) container_sum_query = f\"sum by (pod_name, container_namespace) (irate(kepler_container_joules_total{{container_namespace={container_namespace}, pod_name={pod_name}}}[{time_range_sec}s] @ {str(to_timestamp)}))\" container_core_query = f\"sum by (pod_name, container_namespace) (irate(kepler_container_core_joules_total{{container_namespace={container_namespace}, pod_name={pod_name}}}[{time_range_sec}s] @ {str(to_timestamp)}))\" container_uncore_query = f\"sum by (pod_name, container_namespace) (irate(kepler_container_uncore_joules_total{{container_namespace={container_namespace}, pod_name={pod_name}}}[{time_range_sec}s] @ {str(to_timestamp)}))\" container_pkg_query = f\"sum by (pod_name, container_namespace) (irate(kepler_container_package_joules_total{{container_namespace={container_namespace}, pod_name={pod_name}}}[{time_range_sec}s] @ {str(to_timestamp)}))\" container_dram_query = f\"sum by (pod_name, container_namespace) (irate(kepler_container_dram_joules_total{{container_namespace={container_namespace}, pod_name={pod_name}}}[{time_range_sec}s] @ {str(to_timestamp)}))\" container_other_query = f\"sum by (pod_name, container_namespace) (irate(kepler_container_other_joules_total{{container_namespace={container_namespace}, pod_name={pod_name}}}[{time_range_sec}s] @ {str(to_timestamp)}))\" container_gpu_query = f\"sum by (pod_name, container_namespace) (irate(kepler_container_gpu_joules_total{{container_namespace={container_namespace}, pod_name={pod_name}}}[{time_range_sec}s] @ {str(to_timestamp)}))\" sum_data = prom.custom_query(query=container_sum_query) core_data = prom.custom_query(query=container_core_query) uncore_data = prom.custom_query(query=container_uncore_query) pkg_data = prom.custom_query(query=container_pkg_query) dram_data = prom.custom_query(query=container_dram_query) other_data = prom.custom_query(query=container_other_query) gpu_data = prom.custom_query(query=container_gpu_query) metrics = { \"from\": datetime.fromtimestamp(from_timestamp).strftime(\"%m/%d/%Y, %H:%M:%S\"), \"to\": datetime.fromtimestamp(to_timestamp).strftime(\"%m/%d/%Y, %H:%M:%S\"), \"sum\": float(sum_data[0][\"value\"][1]), \"core\": float(core_data[0][\"value\"][1]), \"uncore\": float(uncore_data[0][\"value\"][1]), \"pkg\": float(pkg_data[0][\"value\"][1]), \"dram\": float(dram_data[0][\"value\"][1]), \"other\": float(other_data[0][\"value\"][1]), \"gpu\": float(gpu_data[0][\"value\"][1]), } return metrics >>> # prometheus has UTC 00:00 as opposed to Barcelona UTC +02:00 >>> to_timestamp = datetime(2024, 6, 11, 8, 31).timestamp() >>> from_timestamp = datetime(2024, 6, 11, 8, 25).timestamp() >>> >>> get_kepler_pod_stats(to_timestamp=to_timestamp, from_timestamp=from_timestamp) >>> >>> {'from': '06/11/2024, 08:25:00', >>> 'to': '06/11/2024, 08:31:00', >>> 'sum': 45.333333333333336, >>> 'core': 0.0, >>> 'uncore': 0.0, >>> 'pkg': 44.48569999999987, >>> 'dram': 0.8289999999998447, >>> 'other': 0.0, >>> 'gpu': 0.0} Carbon emission monitoring \u00b6 Grafana dashboard provided by Kepler uses predefined natural gas/coal/petroleum conversions from US . See simple implemented formula here with coefficient defined here . The Energy measurements can be easily transformed to Carbon emissions using CodeCarbon methodology . Code \u00b6 RAM/CPU/GPU \u00b6 All-in-one solution by Scalene: https://github.com/plasma-umass/scalene Carbon emission monitoring: \u00b6 https://mlco2.github.io/codecarbon/usage.html# https://github.com/sb-ai-lab/Eco2AI https://github.com/lfwa/carbontracker Code Execution timing \u00b6 Timer decorator Import and prepend the time decorator to log time(into specified log file) that it takes to execute analyzed function/class, e.g.: import logging from functools import wraps from time import time def timer(func): \"\"\"Wrapper to time and log the function execution. Parameters: func: function \"\"\" @wraps(func) def timer_func(*args, **kwargs): start_time = time() value = func(*args, **kwargs) end_time = time() logging.info( f\"Finished {func.__name__} in {(end_time - start_time):.4f} seconds.\" # noqa ) return value return timer_func @timer def training(config: TrainingConfig, custom_params: CustomParameters): [Legacy] RAM \u00b6 Max RAM To get max RAM used during executition of the function import the decorator and put it above it function to log max RAM used into specified logging file, e.g.: NOTE : this applies only to functions and NOT classes or class methods. import logging from functools import wraps from memory_profiler import memory_usage def ram_usage(func): \"\"\"Wrapper to monitor and log RAM usage during the function execution. NOTE: can be applied to function but not to method of a class. Parameters: func: function \"\"\" @wraps(func) def ram_usage_func(*args, **kwargs): ram, value = memory_usage( (func, args, *kwargs), interval=1.0, retval=True, max_usage=True ) logging.info( f\"Finished {func.__name__,}. Max RAM used {(ram / 1000):.4f} GB.\" ) # noqa return value return ram_usage_func from churn_pred.code_profiling import ram_usage @ram_usage def training(config: TrainingConfig, custom_params: CustomParameters): Per line RAM usage analysis To analyze RAM usage per line of the code import profile decorator, put it above the function you want to analyze: from memory_profiler import profile @profile def training(config: TrainingConfig, custom_params: CustomParameters): , run the code using memory_profile command line util, and analyze the RAM usage, e.g.: $ python -m memory_profiler local_run.py Filename: /XXX/main.py Line # Mem usage Increment Occurrences Line Contents ============================================================= 53 230.8 MiB 230.8 MiB 1 @ram_usage 54 @profile 55 def training(config: TrainingConfig, custom_params: CustomParameters): 56 230.9 MiB 0.1 MiB 1 data_config = DataConfig.load(config.data_config) 57 230.9 MiB 0.0 MiB 1 hyperparameters = Hyperparameters.parse_obj(config.hyperparameters) 58 59 687.5 MiB 456.6 MiB 1 queries, df_data = get_data(config, custom_params) 60 687.5 MiB 0.0 MiB 1 (","title":"code profiling"},{"location":"code_profiling.html#code-profiling-5-min-read","text":"","title":"Code profiling [5 min read]"},{"location":"code_profiling.html#jupyterhubany-kubernetes-pod","text":"","title":"JupyterHub(any Kubernetes) pod"},{"location":"code_profiling.html#code","text":"","title":"Code"},{"location":"contacts.html","text":"Contacts [1 min read] \u00b6 SAI Research Unit lead: Paolo Dini; pdini@cttc.es Kubernetes administration: Pavol Mulinka, pmulinka@cttc.es Hardware related questions: Marco Miozzo; marco.miozzo@cttc.cat Jordi Serra; jserra@cttc.es Network related queries and requests: Centre de Serveis Inform\u00e0tics (CSI); csi@cttc.es ; GLPI ticketing system","title":"contacts"},{"location":"contacts.html#contacts-1-min-read","text":"SAI Research Unit lead: Paolo Dini; pdini@cttc.es Kubernetes administration: Pavol Mulinka, pmulinka@cttc.es Hardware related questions: Marco Miozzo; marco.miozzo@cttc.cat Jordi Serra; jserra@cttc.es Network related queries and requests: Centre de Serveis Inform\u00e0tics (CSI); csi@cttc.es ; GLPI ticketing system","title":"Contacts [1 min read]"},{"location":"contributing.html","text":"Documentation \u00b6 Documentation is created using github pages and mkdocs, see # to build locally cd docs pip install -r requirements.txt mkdocs build # to push to github pages mkdocs gh-deploy # if you want to run webserver locally mkdocs serve","title":"Documentation"},{"location":"contributing.html#documentation","text":"Documentation is created using github pages and mkdocs, see # to build locally cd docs pip install -r requirements.txt mkdocs build # to push to github pages mkdocs gh-deploy # if you want to run webserver locally mkdocs serve","title":"Documentation"},{"location":"jupyterhub.html","text":"Zero-to-Jupyterhub [3 min read] \u00b6 The purpose of this document is to describe the possible ways of accessing SAI CTTC computational resources available through JupyterHub. Access \u00b6 Requesting Access \u00b6 prerequisite: have a GitHub account contact Pavol Mulinka to add you to https://github.com/sai-cttc group change your membership to public https://github.com/orgs/sai-cttc/people External \u00b6 WebGUI \u00b6 login and test spawning a JupyterHub pod at https://sai-jupyterhub.cttc.es Internal(CTTC VPN) \u00b6 WebGUI \u00b6 login and spawn a JupyterHub pod at any of the following https://10.1.24.200 https://jupyterhub.sai.kubernetes.local https://sai-jupyterhub.cttc.es SSH \u00b6 generate token in JupyterHub GUI ssh using your Jupyterhub/GitHub username on port 22022 on either local IP address or local domain name jupyterhub.sai.kubernetes.local and use generated token as password: VSCode remote ssh \u00b6 Please follow the official VSCode instructions: https://code.visualstudio.com/docs/remote/ssh * to generate a token, follow the instructions in the SSH section VSCode-Server \u00b6 Please follow the official VSCode-server instructions: https://github.com/coder/code-server * to generate a token, follow the instructions in the SSH section Remote Jupyter server \u00b6 Please follow this guide https://blog.jupyter.org/connect-to-a-jupyterhub-from-visual-studio-code-ed7ed3a31bcb . * to generate a token, follow the instructions in the SSH section Keep in mind that: * you are looking at the local code * you are running the code remotely * you are opening files that are present on the remote NOT you local machine Known Issues \u00b6 WebGUI - Internal(CTTC VPN) \u00b6 authentication through internal(local) IP address/URL results in OAuth error, this is most probably due to the following JupyterHub setting: hub: config: GitHubOAuthenticator: oauth_callback_url: https://sai-jupyterhub.cttc.es/hub/oauth_callback VSCode remote ssh \u00b6 At the time of the writing this guide the vscode remote-ssh extension cause the spawned machine to hang, making it unresponsive. I had to restart the spawned machine in WebGUI to regain access to it. SSH Putty is sending JupyterHub token symbols in different encoding than Linux/WSL terminal. Logs in jupyter-ssh pod show \u201cscrambled\u201d symbols while attempting to authenticate ssh session SSH in Linux/WSL terminal prepends and appends additional symbols to JupyterHub token. These symbols are removed in code running in jupyterhub-ssh pod. Remote Jupyter server \u00b6 At the time of writing this guide I lost this feature in VScode for unknown reason and I was unable to test it. I tried running VScode under WSL and Windows, both \u201cnormal\u201d and development version.","title":"jupyterhub"},{"location":"jupyterhub.html#zero-to-jupyterhub-3-min-read","text":"The purpose of this document is to describe the possible ways of accessing SAI CTTC computational resources available through JupyterHub.","title":"Zero-to-Jupyterhub [3 min read]"},{"location":"jupyterhub.html#access","text":"","title":"Access"},{"location":"jupyterhub.html#known-issues","text":"","title":"Known Issues"},{"location":"kubernetes_services.html","text":"Services [14 min read] \u00b6 Summary \u00b6 Key Value Grafana GUI grafana.kubernetes.local.cttc.es Grafana GUI(port-forward) ssh -L 3000:localhost:3000 pmulinka@10.1.24.200 kubectl port-forward svc/prometheus-grafana 3000:80 -n monitoring http://localhost:3000 Prometheus GUI(port-forward) ssh -L 3000:localhost:3000 pmulinka@10.1.24.200 kubectl port-forward svc/prometheus-kube-prometheus-prometheus 3000:9090 -n monitoring http://localhost:3000/graph Alermanager GUI(port-forward) ssh -L 3000:localhost:3000 pmulinka@10.1.24.200 kubectl port-forward svc/prometheus-kube-prometheus-alertmanager 3000:9093 -n monitoring http://localhost:3000 MLflow GUI(Nodeport) http://10.1.24.50:30580 MLflow GUI(port-forward) ssh -L 3000:localhost:3000 pmulinka@10.1.24.200 kubectl port-forward svc/mlflow 3000:5000 -n mlflow http://localhost:3000 MLflow host(cluster) 10.152.183.54 MLflow host url(cluster) mlflow.mlflow.svc.cluster.local MLflow port 5000 Minio GUI(Nodeport - but not working?) http://10.1.24.50:31747 Minio GUI(port-forward) ssh -L 3000:localhost:3000 pmulinka@10.1.24.200 kubectl port-forward svc/minio 3000:9001 -n minio http://localhost:3000 Minio host(cluster) 10.152.183.135 Minio host url(cluster) minio-operator.microk8s-console.svc.cluster.local Minio port 9090 Minio awsAccessKeyId minioadmin Minio awsSecretAccessKey minioadmin Minio bucket mlflow MySQL CMD mysql -h 10.152.183.77 --user mlflow --password mlflow status MySQL host(cluster) 10.152.183.77 MySQL host url(cluster) mysql.mysql.svc.cluster.local MySQL port 3306 MySQL user mlflow MySQL password mlflow MySQL database mlflow Core services \u00b6 Prometheus stack \u00b6 Prometheus, Grafana, Alertmanager, Thanos-? add Helm repo helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update install Prometheus, Kepler [prometheus-stack](https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/README.md) with defined values helm install prometheus-stack prometheus-community/kube-prometheus-stack --namespace prometheus-stack --create-namespace -f configs/prometheus_stack.yaml make the grafana and prometheus service available from outside using ingress kubectl apply -f configs/ingress_prometheus_stack.yaml Kepler \u00b6 add Helm repo helm repo add kepler https://sustainable-computing-io.github.io/kepler-helm-chart helm repo update install [kepler](https://sustainable-computing.io/installation/kepler-helm/) - default confguration helm install kepler kepler/kepler --namespace kepler --create-namespace scrape Kepler metrics by Prometheus kubectl apply -f configs/prometheus_kepler_service_monitor.yaml import Kepler dashboard into Grafana [Kepler dashboard](/configs/Kepler_Exporter_dashboard.json) dashboard Inference Services \u00b6 MLflow \u00b6 implemented - not the most up to date solution, but works and the mlflow pod was spawned without issues as opposed to bitnami mlflow package install helm install mlflow community-charts/mlflow \\ --set service.type=NodePort \\ --set backendStore.databaseMigration=true \\ --set backendStore.mysql.enabled=true \\ --set backendStore.mysql.host=mysql.mysql.svc.cluster.local \\ --set backendStore.mysql.port=3306 \\ --set backendStore.mysql.database=mlflow \\ --set backendStore.mysql.user=mlflow \\ --set backendStore.mysql.password=mlflow \\ --set artifactRoot.s3.enabled=true \\ --set artifactRoot.s3.bucket=mlflow \\ --set artifactRoot.s3.awsAccessKeyId=minioadmin \\ --set artifactRoot.s3.awsSecretAccessKey=minioadmin \\ --set extraEnvVars.MLFLOW_S3_ENDPOINT_URL=http://10.152.183.156:9000 \\ --set serviceMonitor.enabled=true \\ --namespace mlflow --create-namespace output Release \"mlflow\" has been upgraded. Happy Helming! NAME: mlflow LAST DEPLOYED: Thu May 16 15:24:32 2024 NAMESPACE: mlflow STATUS: deployed REVISION: 3 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace mlflow -o jsonpath=\"{.spec.ports[0].nodePort}\" services mlflow) export NODE_IP=$(kubectl get nodes --namespace mlflow -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT NOT implemented Bitnami MLflow package - at the time of the implementation the spawned mlflow pod was crashing with unhelpful logs(no logs saved, sorry) Minio \u00b6 Implemented install helm install minio oci://registry-1.docker.io/bitnamicharts/minio \\ --set service.type=NodePort \\ --set auth.rootUser=minioadmin \\ --set auth.rootPassword=minioadmin \\ --namespace minio --create-namespace output Pulled: registry-1.docker.io/bitnamicharts/minio:14.4.2 Digest: sha256:cee339fbfbb55ff08aa1a9e3abdc01fa9fb90094a49709873fe8ee3e3efb352c NAME: minio LAST DEPLOYED: Wed May 15 15:40:15 2024 NAMESPACE: minio STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: CHART NAME: minio CHART VERSION: 14.4.2 APP VERSION: 2024.5.10 ** Please be patient while the chart is being deployed ** MinIO&reg; can be accessed via port on the following DNS name from within your cluster: minio.minio.svc.cluster.local To get your credentials run: export ROOT_USER=$(kubectl get secret --namespace minio minio -o jsonpath=\"{.data.root-user}\" | base64 -d) export ROOT_PASSWORD=$(kubectl get secret --namespace minio minio -o jsonpath=\"{.data.root-password}\" | base64 -d) To connect to your MinIO&reg; server using a client: - Run a MinIO&reg; Client pod and append the desired command (e.g. 'admin info'): kubectl run --namespace minio minio-client \\ --rm --tty -i --restart='Never' \\ --env MINIO_SERVER_ROOT_USER=$ROOT_USER \\ --env MINIO_SERVER_ROOT_PASSWORD=$ROOT_PASSWORD \\ --env MINIO_SERVER_HOST=minio \\ --image docker.io/bitnami/minio-client:2024.5.9-debian-12-r2 -- admin info minio To access the MinIO&reg; web UI: - Get the MinIO&reg; URL: export NODE_PORT=$(kubectl get --namespace minio -o jsonpath=\"{.spec.ports[0].nodePort}\" services minio) export NODE_IP=$(kubectl get nodes --namespace minio -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo \"MinIO&reg; web URL: http://$NODE_IP:$NODE_PORT/minio\" WARNING: There are \"resources\" sections in the chart not set. Using \"resourcesPreset\" is not recommended for production. For production installations, please set the following values according to your workload needs: - resources +info https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ issues Kserve inference service issues Issues saying that Kserve could not locate credentials: botocore.exceptions.NoCredentialsError: Unable to locate credentials , I tried to remove them, but this did not help: helm upgrade minio oci://registry-1.docker.io/bitnamicharts/minio \\ --set service.type=NodePort \\ --set auth.rootUser=admin \\ --set auth.rootPassword=\"\" \\ --namespace minio NOT implemented Minio operator [The operator](https://min.io/docs/minio/kubernetes/upstream/operations/install-deploy-manage/deploy-operator-helm.html) is for more complex deployments. helm repo add minio-operator https://operator.min.io helm install --namespace minio-operator --create-namespace operator minio-operator/operator Minio microk8s addon * [Minio microk8s addon](https://microk8s.io/docs/addon-minio) install: sudo microk8s enable minio -c 30Gi -s nfs Infer repository core for addon minio Infer repository core for addon dns Addon core/dns is already enabled Infer repository core for addon hostpath-storage Addon core/hostpath-storage is already enabled Download kubectl-minio % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 100 36.8M 100 36.8M 0 0 13.8M 0 0:00:02 0:00:02 --:--:-- 18.1M Initialize minio operator Warning: resource namespaces/minio-operator is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically. namespace/minio-operator configured serviceaccount/minio-operator created clusterrole.rbac.authorization.k8s.io/minio-operator-role created clusterrolebinding.rbac.authorization.k8s.io/minio-operator-binding created customresourcedefinition.apiextensions.k8s.io/tenants.minio.min.io created service/operator created deployment.apps/minio-operator created serviceaccount/console-sa created secret/console-sa-secret created clusterrole.rbac.authorization.k8s.io/console-sa-role created clusterrolebinding.rbac.authorization.k8s.io/console-sa-binding created configmap/console-env created service/console created deployment.apps/console created ----------------- To open Operator UI, start a port forward using this command: kubectl minio proxy -n minio-operator ----------------- Create default tenant with: Name: microk8s Capacity: 30Gi Servers: 1 Volumes: 1 Storage class: nfs TLS: no Prometheus: no + /var/snap/microk8s/common/plugins/kubectl-minio tenant create microk8s --storage-class nfs --capacity 30Gi --servers 1 --volumes 1 --namespace minio-operator --enable-audit-logs=false --disable-tls --enable-prometheus=false W0513 11:19:46.012386 2934999 warnings.go:70] unknown field \"spec.pools[0].volumeClaimTemplate.metadata.creationTimestamp\" Tenant 'microk8s' created in 'minio-operator' Namespace Username: 6ZQD4KM2Z4S952HYL73M Password: vLDbSJ1C6cXKuGLC2K4V5wigatpCfjiICZY3owKM Note: Copy the credentials to a secure location. MinIO will not display these again. APPLICATION SERVICE NAME NAMESPACE SERVICE TYPE SERVICE PORT MinIO minio minio-operator ClusterIP 80 Console microk8s-console minio-operator ClusterIP 9090 + set +x ================================ Enabled minio addon. You can manage minio tenants using the kubectl-minio plugin. For more details, use Minio addon is not working anymore in microk8s - [git issue](https://github.com/minio/console/issues/3318), output in Kubernetes cluster: Normal Scheduled 3m4s default-scheduler Successfully assigned minio-operator/console-78d567bfc8-gsspn to iesc-gpu Normal Pulling 89s (x4 over 3m4s) kubelet Pulling image \"minio/console:v0.20.3\" Warning Failed 88s (x4 over 3m2s) kubelet Failed to pull image \"minio/console:v0.20.3\": failed to pull and unpack image \"docker.io/minio/console:v0.20.3\": failed to resolve reference \"docker.io/minio/console:v0.20.3\": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed Warning Failed 88s (x4 over 3m2s) kubelet Error: ErrImagePull Warning Failed 74s (x6 over 3m1s) kubelet Error: ImagePullBackOff Normal BackOff 60s (x7 over 3m1s) kubelet Back-off pulling image \"minio/console:v0.20.3\" MySQL \u00b6 Implemented install helm install mysql oci://registry-1.docker.io/bitnamicharts/mysql \\ --set auth.database=mlflow \\ --set auth.username=mlflow \\ --set auth.password=mlflow \\ --set auth.rootPassword=root \\ --namespace mysql --create-namespace output Pulled: registry-1.docker.io/bitnamicharts/mysql:10.2.2 Digest: sha256:61b5d1a6f8ac29662160d30e620ed388d782857e9d895585181a6930c83f1ebf NAME: mysql LAST DEPLOYED: Mon May 13 11:54:19 2024 NAMESPACE: mysql STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: CHART NAME: mysql CHART VERSION: 10.2.2 APP VERSION: 8.0.37 ** Please be patient while the chart is being deployed ** Tip: Watch the deployment status using the command: kubectl get pods -w --namespace mysql Services: echo Primary: mysql.mysql.svc.cluster.local:3306 Execute the following to get the administrator credentials: echo Username: root MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace mysql mysql -o jsonpath=\"{.data.mysql-root-password}\" | base64 -d) To connect to your database: 1. Run a pod that you can use as a client: kubectl run mysql-client --rm --tty -i --restart='Never' --image docker.io/bitnami/mysql:8.0.37-debian-12-r0 --namespace mysql --env MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD --command -- bash 2. To connect to primary service (read/write): mysql -h mysql.mysql.svc.cluster.local -uroot -p\"$MYSQL_ROOT_PASSWORD\" WARNING: There are \"resources\" sections in the chart not set. Using \"resourcesPreset\" is not recommended for production. For production installations, please set the following values according to your workload needs: - primary.resources - secondary.resources +info https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ NOT implemented MySQL operator [The operator](https://dev.mysql.com/doc/mysql-operator/en/mysql-operator-installation-helm.html) is for more complex mysql deployments using InnoDB cluster helm repo add MySQL-operator https://mysql.github.io/mysql-operator/ helm install my-mysql-operator mysql-operator/mysql-operator --namespace mysql-operator --create-namespace Kserve \u00b6 I chose Serverless implementation as it supports Scale down and from Zero and I do not need Mesh implementation with multiple models in a pod. Kserve supports node selector, node affinity and tolerations to select edge nodes for deployment of the model. I could not find the sam capabilities for Seldom Core Knative \u00b6 Kserve installation guide image verification did not work so I skipped it - cosign was not working sudo apt install golang-go sudo apt install -y jq go install github.com/sigstore/cosign/v2/cmd/cosign@latest Istio (for Knative and Kserve) \u00b6 Istio is recommended for Kserve Installation pmulinka@saiacheron:~/kubernetes/knative$ kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.14.0/serving-crds.yaml customresourcedefinition.apiextensions.k8s.io/certificates.networking.internal.knative.dev created customresourcedefinition.apiextensions.k8s.io/configurations.serving.knative.dev created customresourcedefinition.apiextensions.k8s.io/clusterdomainclaims.networking.internal.knative.dev created customresourcedefinition.apiextensions.k8s.io/domainmappings.serving.knative.dev created customresourcedefinition.apiextensions.k8s.io/ingresses.networking.internal.knative.dev created customresourcedefinition.apiextensions.k8s.io/metrics.autoscaling.internal.knative.dev created customresourcedefinition.apiextensions.k8s.io/podautoscalers.autoscaling.internal.knative.dev created customresourcedefinition.apiextensions.k8s.io/revisions.serving.knative.dev created customresourcedefinition.apiextensions.k8s.io/routes.serving.knative.dev created customresourcedefinition.apiextensions.k8s.io/serverlessservices.networking.internal.knative.dev created customresourcedefinition.apiextensions.k8s.io/services.serving.knative.dev created customresourcedefinition.apiextensions.k8s.io/images.caching.internal.knative.dev created pmulinka@saiacheron:~/kubernetes/knative$ kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.14.0/serving-core.yaml namespace/knative-serving created role.rbac.authorization.k8s.io/knative-serving-activator created clusterrole.rbac.authorization.k8s.io/knative-serving-activator-cluster created clusterrole.rbac.authorization.k8s.io/knative-serving-aggregated-addressable-resolver created clusterrole.rbac.authorization.k8s.io/knative-serving-addressable-resolver created clusterrole.rbac.authorization.k8s.io/knative-serving-namespaced-admin created clusterrole.rbac.authorization.k8s.io/knative-serving-namespaced-edit created clusterrole.rbac.authorization.k8s.io/knative-serving-namespaced-view created clusterrole.rbac.authorization.k8s.io/knative-serving-core created clusterrole.rbac.authorization.k8s.io/knative-serving-podspecable-binding created serviceaccount/controller created clusterrole.rbac.authorization.k8s.io/knative-serving-admin created clusterrolebinding.rbac.authorization.k8s.io/knative-serving-controller-admin created clusterrolebinding.rbac.authorization.k8s.io/knative-serving-controller-addressable-resolver created serviceaccount/activator created rolebinding.rbac.authorization.k8s.io/knative-serving-activator created clusterrolebinding.rbac.authorization.k8s.io/knative-serving-activator-cluster created customresourcedefinition.apiextensions.k8s.io/images.caching.internal.knative.dev unchanged certificate.networking.internal.knative.dev/routing-serving-certs created customresourcedefinition.apiextensions.k8s.io/certificates.networking.internal.knative.dev unchanged customresourcedefinition.apiextensions.k8s.io/configurations.serving.knative.dev unchanged customresourcedefinition.apiextensions.k8s.io/clusterdomainclaims.networking.internal.knative.dev unchanged customresourcedefinition.apiextensions.k8s.io/domainmappings.serving.knative.dev unchanged customresourcedefinition.apiextensions.k8s.io/ingresses.networking.internal.knative.dev unchanged customresourcedefinition.apiextensions.k8s.io/metrics.autoscaling.internal.knative.dev unchanged customresourcedefinition.apiextensions.k8s.io/podautoscalers.autoscaling.internal.knative.dev unchanged customresourcedefinition.apiextensions.k8s.io/revisions.serving.knative.dev unchanged customresourcedefinition.apiextensions.k8s.io/routes.serving.knative.dev unchanged customresourcedefinition.apiextensions.k8s.io/serverlessservices.networking.internal.knative.dev unchanged customresourcedefinition.apiextensions.k8s.io/services.serving.knative.dev unchanged image.caching.internal.knative.dev/queue-proxy created configmap/config-autoscaler created configmap/config-defaults created configmap/config-deployment created configmap/config-domain created configmap/config-features created configmap/config-gc created configmap/config-leader-election created configmap/config-logging created configmap/config-network created configmap/config-observability created configmap/config-tracing created horizontalpodautoscaler.autoscaling/activator created poddisruptionbudget.policy/activator-pdb created deployment.apps/activator created service/activator-service created deployment.apps/autoscaler created service/autoscaler created deployment.apps/controller created service/controller created horizontalpodautoscaler.autoscaling/webhook created poddisruptionbudget.policy/webhook-pdb created deployment.apps/webhook created service/webhook created validatingwebhookconfiguration.admissionregistration.k8s.io/config.webhook.serving.knative.dev created mutatingwebhookconfiguration.admissionregistration.k8s.io/webhook.serving.knative.dev created validatingwebhookconfiguration.admissionregistration.k8s.io/validation.webhook.serving.knative.dev created secret/webhook-certs created pmulinka@saiacheron:~/kubernetes/knative$ kubectl apply -l knative.dev/crd-install=true -f https://github.com/knative/net-istio/releases/download/knative-v1.14.0/istio.yaml customresourcedefinition.apiextensions.k8s.io/authorizationpolicies.security.istio.io created customresourcedefinition.apiextensions.k8s.io/destinationrules.networking.istio.io created customresourcedefinition.apiextensions.k8s.io/envoyfilters.networking.istio.io created customresourcedefinition.apiextensions.k8s.io/gateways.networking.istio.io created customresourcedefinition.apiextensions.k8s.io/peerauthentications.security.istio.io created customresourcedefinition.apiextensions.k8s.io/proxyconfigs.networking.istio.io created customresourcedefinition.apiextensions.k8s.io/requestauthentications.security.istio.io created customresourcedefinition.apiextensions.k8s.io/serviceentries.networking.istio.io created customresourcedefinition.apiextensions.k8s.io/sidecars.networking.istio.io created customresourcedefinition.apiextensions.k8s.io/telemetries.telemetry.istio.io created customresourcedefinition.apiextensions.k8s.io/virtualservices.networking.istio.io created customresourcedefinition.apiextensions.k8s.io/wasmplugins.extensions.istio.io created customresourcedefinition.apiextensions.k8s.io/workloadentries.networking.istio.io created customresourcedefinition.apiextensions.k8s.io/workloadgroups.networking.istio.io created pmulinka@saiacheron:~/kubernetes/knative$ kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.14.0/istio.yaml namespace/istio-system created serviceaccount/istio-ingressgateway-service-account created serviceaccount/istio-reader-service-account created serviceaccount/istiod created clusterrole.rbac.authorization.k8s.io/istio-reader-clusterrole-istio-system created clusterrole.rbac.authorization.k8s.io/istiod-clusterrole-istio-system created clusterrole.rbac.authorization.k8s.io/istiod-gateway-controller-istio-system created clusterrolebinding.rbac.authorization.k8s.io/istio-reader-clusterrole-istio-system created clusterrolebinding.rbac.authorization.k8s.io/istiod-clusterrole-istio-system created clusterrolebinding.rbac.authorization.k8s.io/istiod-gateway-controller-istio-system created role.rbac.authorization.k8s.io/istio-ingressgateway-sds created role.rbac.authorization.k8s.io/istiod created rolebinding.rbac.authorization.k8s.io/istio-ingressgateway-sds created rolebinding.rbac.authorization.k8s.io/istiod created customresourcedefinition.apiextensions.k8s.io/authorizationpolicies.security.istio.io unchanged customresourcedefinition.apiextensions.k8s.io/destinationrules.networking.istio.io unchanged customresourcedefinition.apiextensions.k8s.io/envoyfilters.networking.istio.io unchanged customresourcedefinition.apiextensions.k8s.io/gateways.networking.istio.io unchanged customresourcedefinition.apiextensions.k8s.io/peerauthentications.security.istio.io unchanged customresourcedefinition.apiextensions.k8s.io/proxyconfigs.networking.istio.io unchanged customresourcedefinition.apiextensions.k8s.io/requestauthentications.security.istio.io unchanged customresourcedefinition.apiextensions.k8s.io/serviceentries.networking.istio.io unchanged customresourcedefinition.apiextensions.k8s.io/sidecars.networking.istio.io unchanged customresourcedefinition.apiextensions.k8s.io/telemetries.telemetry.istio.io unchanged customresourcedefinition.apiextensions.k8s.io/virtualservices.networking.istio.io unchanged customresourcedefinition.apiextensions.k8s.io/wasmplugins.extensions.istio.io unchanged customresourcedefinition.apiextensions.k8s.io/workloadentries.networking.istio.io unchanged customresourcedefinition.apiextensions.k8s.io/workloadgroups.networking.istio.io unchanged configmap/istio created configmap/istio-sidecar-injector created deployment.apps/istio-ingressgateway created deployment.apps/istiod created service/istio-ingressgateway created service/istiod created horizontalpodautoscaler.autoscaling/istiod created poddisruptionbudget.policy/istio-ingressgateway created poddisruptionbudget.policy/istiod created mutatingwebhookconfiguration.admissionregistration.k8s.io/istio-sidecar-injector created validatingwebhookconfiguration.admissionregistration.k8s.io/istio-validator-istio-system created pmulinka@saiacheron:~/kubernetes/knative$ pmulinka@saiacheron:~/kubernetes/knative$ kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.14.0/net-istio.yaml clusterrole.rbac.authorization.k8s.io/knative-serving-istio created gateway.networking.istio.io/knative-ingress-gateway created gateway.networking.istio.io/knative-local-gateway created service/knative-local-gateway created configmap/config-istio created peerauthentication.security.istio.io/webhook created peerauthentication.security.istio.io/net-istio-webhook created deployment.apps/net-istio-controller created deployment.apps/net-istio-webhook created secret/net-istio-webhook-certs created service/net-istio-webhook created mutatingwebhookconfiguration.admissionregistration.k8s.io/webhook.istio.networking.internal.knative.dev created validatingwebhookconfiguration.admissionregistration.k8s.io/config.webhook.istio.networking.internal.knative.dev created certificate.networking.internal.knative.dev/routing-serving-certs created pmulinka@saiacheron:~/kubernetes/knative$ pmulinka@saiacheron:~/kubernetes/knative$ kubectl --namespace istio-system get service istio-ingressgateway NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.152.183.36 <pending> 15021:32390/TCP,80:32327/TCP,443:32456/TCP 29s verification pmulinka@saiacheron:~/kubernetes/knative$ kubectl get pods -n knative-serving NAME READY STATUS RESTARTS AGE activator-55d856fccd-5b9jt 1/1 Running 0 3m27s autoscaler-5df8b7c68-4fzm7 1/1 Running 0 3m27s controller-78b7976cc6-h7bz2 1/1 Running 0 3m27s net-istio-controller-cc877c4dc-xxlrj 1/1 Running 0 119s net-istio-webhook-69cd4975b8-9kcv6 1/1 Running 0 119s webhook-7c7b4cd674-kghj9 1/1 Running 0 3m26s pmulinka@saiacheron:~/kubernetes/knative$ kubectl get svc -n knative-serving NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE activator-service ClusterIP 10.152.183.59 <none> 9090/TCP,8008/TCP,80/TCP,81/TCP,443/TCP 3m38s autoscaler ClusterIP 10.152.183.75 <none> 9090/TCP,8008/TCP,8080/TCP 3m38s autoscaler-bucket-00-of-01 ClusterIP 10.152.183.69 <none> 8080/TCP 3m30s controller ClusterIP 10.152.183.166 <none> 9090/TCP,8008/TCP 3m38s net-istio-webhook ClusterIP 10.152.183.76 <none> 9090/TCP,8008/TCP,443/TCP 2m10s webhook ClusterIP 10.152.183.241 <none> 9090/TCP,8008/TCP,443/TCP 3m37s Cert Manager \u00b6 Using Microk8s addon . install pmulinka@saiacheron:~/kubernetes/knative$ microk8s enable cert-manager Infer repository core for addon cert-manager Enable DNS addon Infer repository core for addon dns Addon core/dns is already enabled Enabling cert-manager namespace/cert-manager created customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created serviceaccount/cert-manager-cainjector created serviceaccount/cert-manager created serviceaccount/cert-manager-webhook created configmap/cert-manager-webhook created clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created clusterrole.rbac.authorization.k8s.io/cert-manager-view created clusterrole.rbac.authorization.k8s.io/cert-manager-edit created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created clusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created clusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created role.rbac.authorization.k8s.io/cert-manager:leaderelection created role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created service/cert-manager created service/cert-manager-webhook created deployment.apps/cert-manager-cainjector created deployment.apps/cert-manager created deployment.apps/cert-manager-webhook created mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created Waiting for cert-manager to be ready. ..ready Enabled cert-manager =========================== Cert-manager is installed. As a next step, try creating a ClusterIssuer for Let's Encrypt by creating the following resource: $ microk8s kubectl apply -f - <<EOF --- apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt spec: acme: # You must replace this email address with your own. # Let's Encrypt will use this to contact you about expiring # certificates, and issues related to your account. email: me@example.com server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: # Secret resource that will be used to store the account's private key. name: letsencrypt-account-key # Add a single challenge solver, HTTP01 using nginx solvers: - http01: ingress: class: public EOF Then, you can create an ingress to expose 'my-service:80' on 'https://my-service.example.com' with: $ microk8s enable ingress $ microk8s kubectl create ingress my-ingress \\ --annotation cert-manager.io/cluster-issuer=letsencrypt \\ --rule 'my-service.example.com/*=my-service:80,tls=my-service-tls' Kserve \u00b6 install Kserve pmulinka@saiacheron:~/kubernetes/knative$ kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.12.0/kserve.yaml namespace/kserve created customresourcedefinition.apiextensions.k8s.io/clusterservingruntimes.serving.kserve.io created customresourcedefinition.apiextensions.k8s.io/clusterstoragecontainers.serving.kserve.io created customresourcedefinition.apiextensions.k8s.io/inferencegraphs.serving.kserve.io created customresourcedefinition.apiextensions.k8s.io/inferenceservices.serving.kserve.io created customresourcedefinition.apiextensions.k8s.io/servingruntimes.serving.kserve.io created customresourcedefinition.apiextensions.k8s.io/trainedmodels.serving.kserve.io created serviceaccount/kserve-controller-manager created role.rbac.authorization.k8s.io/kserve-leader-election-role created clusterrole.rbac.authorization.k8s.io/kserve-manager-role created clusterrole.rbac.authorization.k8s.io/kserve-proxy-role created rolebinding.rbac.authorization.k8s.io/kserve-leader-election-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/kserve-manager-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/kserve-proxy-rolebinding created configmap/inferenceservice-config created secret/kserve-webhook-server-secret created service/kserve-controller-manager-metrics-service created service/kserve-controller-manager-service created service/kserve-webhook-server-service created deployment.apps/kserve-controller-manager created certificate.cert-manager.io/serving-cert created issuer.cert-manager.io/selfsigned-issuer created mutatingwebhookconfiguration.admissionregistration.k8s.io/inferenceservice.serving.kserve.io created validatingwebhookconfiguration.admissionregistration.k8s.io/clusterservingruntime.serving.kserve.io created validatingwebhookconfiguration.admissionregistration.k8s.io/inferencegraph.serving.kserve.io created validatingwebhookconfiguration.admissionregistration.k8s.io/inferenceservice.serving.kserve.io created validatingwebhookconfiguration.admissionregistration.k8s.io/servingruntime.serving.kserve.io created validatingwebhookconfiguration.admissionregistration.k8s.io/trainedmodel.serving.kserve.io created Built-in ClusterServingRuntimes pmulinka@saiacheron:~/kubernetes/knative$ kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.12.0/kserve-cluster-resources.yaml clusterservingruntime.serving.kserve.io/kserve-huggingfaceserver created clusterservingruntime.serving.kserve.io/kserve-lgbserver created clusterservingruntime.serving.kserve.io/kserve-mlserver created clusterservingruntime.serving.kserve.io/kserve-paddleserver created clusterservingruntime.serving.kserve.io/kserve-pmmlserver created clusterservingruntime.serving.kserve.io/kserve-sklearnserver created clusterservingruntime.serving.kserve.io/kserve-tensorflow-serving created clusterservingruntime.serving.kserve.io/kserve-torchserve created clusterservingruntime.serving.kserve.io/kserve-tritonserver created clusterservingruntime.serving.kserve.io/kserve-xgbserver created clusterstoragecontainer.serving.kserve.io/default created Redis DB for possible testing \u00b6 install helm install redis oci://registry-1.docker.io/bitnamicharts/redis --set auth.password=redis --namespace redis --create-namespace output Pulled: registry-1.docker.io/bitnamicharts/redis:19.3.2 Digest: sha256:1eb24b3e230b23cd307e8aa5ef9006a52484c7e3cf1b4b2eb611f113f24e53e5 NAME: redis LAST DEPLOYED: Tue May 14 11:19:39 2024 NAMESPACE: redis STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: CHART NAME: redis CHART VERSION: 19.3.2 APP VERSION: 7.2.4 ** Please be patient while the chart is being deployed ** Redis&reg; can be accessed on the following DNS names from within your cluster: redis-master.redis.svc.cluster.local for read/write operations (port 6379) redis-replicas.redis.svc.cluster.local for read-only operations (port 6379) To get your password run: export REDIS_PASSWORD=$(kubectl get secret --namespace redis redis -o jsonpath=\"{.data.redis-password}\" | base64 -d) To connect to your Redis&reg; server: 1. Run a Redis&reg; pod that you can use as a client: kubectl run --namespace redis redis-client --restart='Never' --env REDIS_PASSWORD=$REDIS_PASSWORD --image docker.io/bitnami/redis:7.2.4-debian-12-r16 --command -- sleep infinity Use the following command to attach to the pod: kubectl exec --tty -i redis-client \\ --namespace redis -- bash 2. Connect using the Redis&reg; CLI: REDISCLI_AUTH=\"$REDIS_PASSWORD\" redis-cli -h redis-master REDISCLI_AUTH=\"$REDIS_PASSWORD\" redis-cli -h redis-replicas To connect to your database from outside the cluster execute the following commands: kubectl port-forward --namespace redis svc/redis-master 6379:6379 & REDISCLI_AUTH=\"$REDIS_PASSWORD\" redis-cli -h 127.0.0.1 -p 6379 WARNING: There are \"resources\" sections in the chart not set. Using \"resourcesPreset\" is not recommended for production. For production installations, please set the following values according to your workload needs: - replica.resources - master.resources +info https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ APPENDIX: \u00b6 Get requirements and limits of the services/pods, e.g. for Redis kubectl get pods redis-master-0 -n redis -o jsonpath='{range .spec.containers[*]}{\"Container Name: \"}{.name}{\"\\n\"}{\"Requests:\"}{.resources.requests}{\"\\n\"}{\"Limits:\"}{.resources.limits}{\"\\n\"}{end}' Container Name: redis Requests:{\"cpu\":\"100m\",\"ephemeral-storage\":\"50Mi\",\"memory\":\"128Mi\"} Limits:{\"cpu\":\"150m\",\"ephemeral-storage\":\"1Gi\",\"memory\":\"192Mi\"} Get Nodeport:port of the service # example for minio kubectl get --namespace minio -o jsonpath=\"{.spec.ports[0].nodePort}\" services minio kubectl get nodes --namespace minio -o jsonpath=\"{.items[0].status.addresses[0].address} Docker image creation from mlflow model # connect to a machine that has docker and access to model storage and add your user to docker group sudo usermod -a -G docker $(whoami) # install the python venv and activate it sudo apt-get install python3.10-venv python3.10 -m venv python310venv source python310venv/bin/activate # install mlflow and boto3 for communication with s3 storage pip install mlflow pip install boto3 # export keys and s3 endpoint export AWS_ACCESS_KEY_ID=minioadmin export AWS_SECRET_ACCESS_KEY=minioadmin export MLFLOW_S3_ENDPOINT_URL=http://10.152.183.156:9000 # build a docker image mlflow models build-docker -m s3://mlflow/1/777cf64c922149a4b77c85987865deb0/artifacts/success6g_model -n 5uperpalo/mlflow-success6g --enable-mlserver # connect to your dockerhub and push the image docker login -u 5uperpalo docker push 5uperpalo/mlflow-success6g","title":"kubernetes_services"},{"location":"kubernetes_services.html#services-14-min-read","text":"","title":"Services [14 min read]"},{"location":"kubernetes_services.html#summary","text":"Key Value Grafana GUI grafana.kubernetes.local.cttc.es Grafana GUI(port-forward) ssh -L 3000:localhost:3000 pmulinka@10.1.24.200 kubectl port-forward svc/prometheus-grafana 3000:80 -n monitoring http://localhost:3000 Prometheus GUI(port-forward) ssh -L 3000:localhost:3000 pmulinka@10.1.24.200 kubectl port-forward svc/prometheus-kube-prometheus-prometheus 3000:9090 -n monitoring http://localhost:3000/graph Alermanager GUI(port-forward) ssh -L 3000:localhost:3000 pmulinka@10.1.24.200 kubectl port-forward svc/prometheus-kube-prometheus-alertmanager 3000:9093 -n monitoring http://localhost:3000 MLflow GUI(Nodeport) http://10.1.24.50:30580 MLflow GUI(port-forward) ssh -L 3000:localhost:3000 pmulinka@10.1.24.200 kubectl port-forward svc/mlflow 3000:5000 -n mlflow http://localhost:3000 MLflow host(cluster) 10.152.183.54 MLflow host url(cluster) mlflow.mlflow.svc.cluster.local MLflow port 5000 Minio GUI(Nodeport - but not working?) http://10.1.24.50:31747 Minio GUI(port-forward) ssh -L 3000:localhost:3000 pmulinka@10.1.24.200 kubectl port-forward svc/minio 3000:9001 -n minio http://localhost:3000 Minio host(cluster) 10.152.183.135 Minio host url(cluster) minio-operator.microk8s-console.svc.cluster.local Minio port 9090 Minio awsAccessKeyId minioadmin Minio awsSecretAccessKey minioadmin Minio bucket mlflow MySQL CMD mysql -h 10.152.183.77 --user mlflow --password mlflow status MySQL host(cluster) 10.152.183.77 MySQL host url(cluster) mysql.mysql.svc.cluster.local MySQL port 3306 MySQL user mlflow MySQL password mlflow MySQL database mlflow","title":"Summary"},{"location":"kubernetes_services.html#core-services","text":"","title":"Core services"},{"location":"kubernetes_services.html#inference-services","text":"","title":"Inference Services"},{"location":"kubernetes_services.html#appendix","text":"Get requirements and limits of the services/pods, e.g. for Redis kubectl get pods redis-master-0 -n redis -o jsonpath='{range .spec.containers[*]}{\"Container Name: \"}{.name}{\"\\n\"}{\"Requests:\"}{.resources.requests}{\"\\n\"}{\"Limits:\"}{.resources.limits}{\"\\n\"}{end}' Container Name: redis Requests:{\"cpu\":\"100m\",\"ephemeral-storage\":\"50Mi\",\"memory\":\"128Mi\"} Limits:{\"cpu\":\"150m\",\"ephemeral-storage\":\"1Gi\",\"memory\":\"192Mi\"} Get Nodeport:port of the service # example for minio kubectl get --namespace minio -o jsonpath=\"{.spec.ports[0].nodePort}\" services minio kubectl get nodes --namespace minio -o jsonpath=\"{.items[0].status.addresses[0].address} Docker image creation from mlflow model # connect to a machine that has docker and access to model storage and add your user to docker group sudo usermod -a -G docker $(whoami) # install the python venv and activate it sudo apt-get install python3.10-venv python3.10 -m venv python310venv source python310venv/bin/activate # install mlflow and boto3 for communication with s3 storage pip install mlflow pip install boto3 # export keys and s3 endpoint export AWS_ACCESS_KEY_ID=minioadmin export AWS_SECRET_ACCESS_KEY=minioadmin export MLFLOW_S3_ENDPOINT_URL=http://10.152.183.156:9000 # build a docker image mlflow models build-docker -m s3://mlflow/1/777cf64c922149a4b77c85987865deb0/artifacts/success6g_model -n 5uperpalo/mlflow-success6g --enable-mlserver # connect to your dockerhub and push the image docker login -u 5uperpalo docker push 5uperpalo/mlflow-success6g","title":"APPENDIX:"},{"location":"openvpn.html","text":"OpenVPN [1 min read] \u00b6 Open VPN server is running on 10.1.24.200:1194 (saiacheron SAI server) Based on instructions from Ubuntu OpenVPN manual . Add new user on a server \u00b6 # create home directory for user and define bash as default shell sudo useradd -m -s /bin/bash USERNAME sudo passwd USERNAME # possibly add user to sudo group sudo usermod -aG sudo USERNAME Create VPN certs on saicheron for external collaborators \u00b6 create certificates sudo -i cd /etc/openvpn/easy-rsa ./easyrsa gen-req USERNAME nopass ./easyrsa sign-req client USERNAME copy, compress (possibly with encryption) certificates mkdir sai_openvpn_certs sudo cp /etc/openvpn/easy-rsa/pki/private/USERNAME.key sai_openvpn_certs/ sudo cp /etc/openvpn/easy-rsa/pki/issued/USERNAME.crt sai_openvpn_certs/ sudo cp /etc/openvpn/easy-rsa/pki/ca.crt sai_openvpn_certs/ sudo cp /etc/openvpn/ta.key tar -czvf sai_openvpn_certs.tar.gz sai_openvpn_certs/ # with encryption # tar -czv sai_openvpn_certs/ | openssl enc -aes-256-cbc -pbkdf2 -iter 100000 -e > sai_openvpn_certs.tar.gz.enc OpenVPN client \u00b6 Follow the instructions here . For Windows: copy all created certificates + adjusted .ovpn to C:\\Program Files\\OpenVPN\\config Allow static source IP on UDP 1194 to sai-jupyterhub.cttc.es \u00b6 Create a ticket in CTTC CSI system to allow the communciation from a static IP address to the OpenVPN server https://glpi.cttc.es/ . Related ticket to open static ports for Optare collaborators: glpi .","title":"openvpn"},{"location":"openvpn.html#openvpn-1-min-read","text":"Open VPN server is running on 10.1.24.200:1194 (saiacheron SAI server) Based on instructions from Ubuntu OpenVPN manual .","title":"OpenVPN [1 min read]"},{"location":"openvpn.html#add-new-user-on-a-server","text":"# create home directory for user and define bash as default shell sudo useradd -m -s /bin/bash USERNAME sudo passwd USERNAME # possibly add user to sudo group sudo usermod -aG sudo USERNAME","title":"Add new user on a server"},{"location":"openvpn.html#create-vpn-certs-on-saicheron-for-external-collaborators","text":"create certificates sudo -i cd /etc/openvpn/easy-rsa ./easyrsa gen-req USERNAME nopass ./easyrsa sign-req client USERNAME copy, compress (possibly with encryption) certificates mkdir sai_openvpn_certs sudo cp /etc/openvpn/easy-rsa/pki/private/USERNAME.key sai_openvpn_certs/ sudo cp /etc/openvpn/easy-rsa/pki/issued/USERNAME.crt sai_openvpn_certs/ sudo cp /etc/openvpn/easy-rsa/pki/ca.crt sai_openvpn_certs/ sudo cp /etc/openvpn/ta.key tar -czvf sai_openvpn_certs.tar.gz sai_openvpn_certs/ # with encryption # tar -czv sai_openvpn_certs/ | openssl enc -aes-256-cbc -pbkdf2 -iter 100000 -e > sai_openvpn_certs.tar.gz.enc","title":"Create VPN certs on saicheron for external collaborators"},{"location":"openvpn.html#openvpn-client","text":"Follow the instructions here . For Windows: copy all created certificates + adjusted .ovpn to C:\\Program Files\\OpenVPN\\config","title":"OpenVPN client"},{"location":"openvpn.html#allow-static-source-ip-on-udp-1194-to-sai-jupyterhubcttces","text":"Create a ticket in CTTC CSI system to allow the communciation from a static IP address to the OpenVPN server https://glpi.cttc.es/ . Related ticket to open static ports for Optare collaborators: glpi .","title":"Allow static source IP on UDP 1194 to sai-jupyterhub.cttc.es"},{"location":"overview.html","text":"Overview [8 min read] \u00b6 Figure 1 Overview of SAI Kubernetes cluster Namespaces \u00b6 Namespaces in Kubernetes provides a mechanism for separating groups of resources within a cluster. Usually, for many users spreaded across multiple teams. I found them also useful in a development, research-based environment. They provide a mechanism for logical separation and clearer, easier way of installing/uninstalling new services. For more information please see documentation on namespaces . Figure 2 Overview of main cluster services Description of namespaces used in SAI Kubernetes implementation \u00b6 kube-system - default Kubernetes administration namespace kube-public - default Kubernetes administration namespace kube-node-lease - default Kubernetes administration namespace sai - namespace reserved for JupyterHub, i.e. the initial(main) reason for Kubernetes implementation metallb-system \u2013 Microk8s load balancer addon namespace, prerequisite for JupyterHub gpu-operator-resources - Microk8s NVIDIA GPU support addon namespace to support spawning of GPU pods in JupyterHub nfs-server-provisioner - Microk8s NFS Ganesha Server and Provisioner addon namespace; by default Microk8s uses Hostpath Storage for provisioning of Persistent Volumes ; this creates an issue if JupyterHub user spawns a pod on machine where he previously did not spawn a pod (e.g. he used CPU pod and now he wants to switch to GPU that is available on different physical machine); NFS solves this problem and was chosen among other network storage protocols due to its simplicity kepler \u2013 Kubernetes Efficient Power Level Exporter (Kepler) namespace monitoring \u2013 kube-prometheus-stack namespace providing services of Prometheus, Grafana and Alert manager, see the related documentation ingress - Microk8s NGINX Ingress Controller addon namespace for forwarding of HTTP, HTTPS, TCP and UDP traffic Services \u00b6 Kubernetes defines services as objects, or methods, for exposing application(or functionality), running on one or more pods, through internal cluster network. Kubernetes Service API exposes functionality of pods through combination of service name and port to overcome issues of pods communication withi cluster using their cluster internal IP addresses. In this documentation we extend this definition to call the service a bundle of services that provides single functionality for the user e.g. JupyterHub, which combines both JupyterHub service and JupyterHub-ssh service. For more information about Kuberntes service please see the documentation . JupyterHub bundle \u00b6 Both JupyterHub and JupyterHub-ssh are deployed in a main sai namespace as initially the purpose of the cluster was to provide ONLY this functionality. JupyterHub (Z ero to JupyterHub doc ): Zero to JupyterHub implementation provides easy implementation of JupyterHub in Kubernetes. The implementation is part of Sustainability in Open Source Projects study, i.e. it is inline with SAI goals to contribute and participate in sustainable AI/ML projects. The implementation documentation provide easy to understand and optimize guidelines using helm charts. JupyterHub-ssh ( GitHub repo ): repository provides a way of access to spawned JupyterHub pods using SSH. The implementation can be extended to use SFTP between user machine and spawned JupyerHub pod. The SFTP extension requires additional configuration and troubleshooting. As the files can be easily uploaded/downloaded from spawned pods using Web GUI, I did not see a need to implement the extension. Figure 3 Communication with and within JupyterHub Kubernetes energy consumption monitoring \u00b6 Bundle of services implemented with focus on energy consumption monitoring of JupyterHub pods. As the implementation includes Prometheus and Grafana the usage of the services extends the initial idea. Figure 4 Kepler metrics export and visualization Kepler Exporter ( helm chart ): a part of Kubernetes-based Efficient Power Level Exporter \u2013 Model server bundle from Sustainable Computing group; it\u2019s a metric exporter utilizing Prometheus to expose power consumption metrics that are visualized in Kepler Grafana dashboard ; the metrics usage could be extended by power consumption estimation or Kubernetes energy efficiency improvement by PEAKS(Power Efficiency Aware Kubernetes Scheduler) and CLEVER(Container Level Energy-efficient VPA Recommender for Kubernetes) Prometheus ( helm chart ): a system and service monitoring system to collect metrics and raise alarms Grafana ( helm chart ): query and visualization platform for monitoring, observation and alarm raising Administration \u00b6 Group of services assuring communication to cluster services, their monitoring and provisioning; including new service deployments. DNS ( microk8s doc ): Service providing Domain Name System functionality for cluster pods NFS ( microk8s doc ): Network File Server functionality for Persistent Volume Claims (PVCs) of JupyterHub user pods enabling user to spawn a pod on any cluster node and have same storage content GPU support ( microk8s doc ): NVIDIA GPU support on cluster nodes and pods to enabling spawing JupyterHub pods utilizing not only node CPUs but also GPUs Dashboard ( microk8s doc ): activity and resource monitoring of the cluster MetalLB ( microk8s doc ): network load balancer that is a prerequisitive for Zero to JupyterHub implementation Alert Manager ( helm chart ): part of Prometheus eco system used to raise system alarms Helm ( microk8s doc ): Kubernetes package manager for easy installation, configuration and optimization of deployments Ingress ( microk8s doc ): NGINX ingress controller serving as a Kubernetes reverse proxy, i.e. to forward HTPP/HTTPS and TCP/UDP traffic to correct cluster services Certificates \u00b6 Administered by Centre de Serveis Inform\u00e0tics (CSI) using Sectigo Certification Authority as CTTC has a contract with them. Certificate is stored in Kubernetes secrets as jupyterhub-tls : sai kubectl get secret -n sai Cluster administration \u00b6 Known Issues and solutions \u00b6 Adding new node to cluster (DNS & GPU) Incorrect DNS search space and DNS was configured on one of the nodes resulting on issues with ingress services relying on correct DNS entries for FQDNS, see commands dns addon GPU addon was not enabled in cluster resulting in issue with spawning JupyterHub pods that use GPUs, enabling GPU addon solved it Forwarding http/https traffic The forwarding of traffic to correct services on correct ports had to be configured according microk8s ingress documentation to enable reaching of the cluster administration services from outside of the cluster Forwarding TCP traffic Jupyterhub-ssh relies on usage of traefik for TCP port forwarding which is enabled in JupyterHub helm chart only if authohttps is enabled; autohttps uses letsencrypt to get certificate for https communication and for that the letsencrypt needs http port allowed on firewall; solution \u2013 manual certificate created by Sectigo(CTTC partner) and Ingress for TCP port forwarding of ssh traffic to jupyterhub-ssh service https certificate verification in jupyterhub-ssh solution manual certificate for jupyterhub is for unknown reason ignored by code from jupyterhub-ssh that is running on the jupyterhub-ssh pod; the certificate is recognized when the code is tested after connecting to a running jupyterhub-ssh pod but it\u2019s ignored by the code running on the background; solution \u2013 after many tries with changing the default user, using root user in the pod etc. I adjusted the code in repository to ignore certificate validation, created and published the docker image to docker hub and used it in the value.yaml configuration file for helm chart installation of jupyterhub-ssh issues and solution related to remote connection to JupyterHub \u2013 see \u201cSAI CTTC JupyterHub User guide\u201d Kubernetes dashboard only accessible with port forwarding and with generated token: I followed the official guide and created security risk by granding admin privileges to dashboard, see the documentation Jupyterhub ssl certificate: official documentation recommends autohttps using letsencrypt(creates a hole in security by enabling http from the internet) or manual cert; manual self-generated cert is an issue for other service and also prompts security alert for access the JupyterHub url; solution: use Sectigo certificate authority as it is a partner of CTTC Jupyterhub unable to spawn pods on newly added cluster node: by default the implementation uses hostpath for user storage, see documentation ; this mean that all user data is stored on node where he spawned his first pod and can\u2019t be moved to other nodes; solution: use NFS Define Jupyterhub pod configuration that can be used: solution by using Kubespawner in Jupyterhub configuration JupyterHub pods being turned of after some time: disable cull option in the Jupyterhub configuration Robust way of user authentication to JupyterHub (i.e. not adjusting the JupyterHub configuration every time someone needs an access): solution by using GitHubOAuthenticator Troubleshooting and other useful commands \u00b6 Troubleshooting \u00b6 Local port forwarding to a specific service: e.g. forward traffic to localhost on port 3000 to a jupyterhub-ssh service on port 22 in sai namespace located in Kubernetes cluster (ip address or FQDN can be of any cluster node) on a local machine ssh -L 3000:localhost:3000 pmulinka@10.1.24.201 on node kubectl port-forward svc/jupyterhub-ssh 3000:22 -n sai Pod status issues kubectl describe pod jupyterhub-ssh-7f58c4bc79-5lg5z Connect to a pod kubectl exec jupyterhub-ssh-7f58c4bc79-5lg5z -n sai -it -- /bin/bash Check pod logs -f for continuous feed kubectl logs jupyterhub-ssh-7f58c4bc79-5lg5z -n sai -f Usefull commands \u00b6 apply configuration to Kubernetes cluster (e.g. apply changes to configuration of the service) kubectl apply -f config.yaml show all running pods, services, deployments kubectl get pods -A kubectl get services -A kubectl get deployments -A","title":"overview"},{"location":"overview.html#overview-8-min-read","text":"Figure 1 Overview of SAI Kubernetes cluster","title":"Overview [8 min read]"},{"location":"overview.html#namespaces","text":"Namespaces in Kubernetes provides a mechanism for separating groups of resources within a cluster. Usually, for many users spreaded across multiple teams. I found them also useful in a development, research-based environment. They provide a mechanism for logical separation and clearer, easier way of installing/uninstalling new services. For more information please see documentation on namespaces . Figure 2 Overview of main cluster services","title":"Namespaces"},{"location":"overview.html#services","text":"Kubernetes defines services as objects, or methods, for exposing application(or functionality), running on one or more pods, through internal cluster network. Kubernetes Service API exposes functionality of pods through combination of service name and port to overcome issues of pods communication withi cluster using their cluster internal IP addresses. In this documentation we extend this definition to call the service a bundle of services that provides single functionality for the user e.g. JupyterHub, which combines both JupyterHub service and JupyterHub-ssh service. For more information about Kuberntes service please see the documentation .","title":"Services"},{"location":"overview.html#certificates","text":"Administered by Centre de Serveis Inform\u00e0tics (CSI) using Sectigo Certification Authority as CTTC has a contract with them. Certificate is stored in Kubernetes secrets as jupyterhub-tls : sai kubectl get secret -n sai","title":"Certificates"},{"location":"overview.html#cluster-administration","text":"","title":"Cluster administration"},{"location":"resource_management.html","text":"Resource management [2 min read] \u00b6 \"Develop small, run big\" Resource usage decision tree JupyterHub | Teams calendar Basic rules to follow \u00b6 if you are using helpfull microservice, deploy it as separate pod and share the usage with team, e.g. Carla or SUCCESS6G services do EDA(Exploratory Data Analysis), develop, debug, and troubleshoot code on small JupyterHub pod or on your local machine run script on larger pod if absolutely necessary run the code on standalone server if ultra-super-duper-absolutely necessary develop solution using SLURM or SLURM on Kubernetes Step-by-step \u00b6 Please follow these simple rules to avoid misuse of the SAI Research Unit computational resources: 1. spawn a small pod that is sufficient to process the data your are using * i.e. depending on the dataset size do a best guess and eventually go bigger 2. profile your code and pod resource usage 3. spawn a larger pod if needed 4. estimate a time and requirements and request a standalone server by putting a note into shared callendar in Teams - MANDATORY * make an ssh message - OPTIONAL (e.g. when you are chasing deadline and cant afford any mishaps) 5. [EXTREME] - develop a solution using multiple machines using SLURM, SLURM on Kubernetes, or other solution; and consult it with team GPU inside JupyterHub pod \u00b6 To get the GPU running in a pod install a proper driver: sudo apt-get update sudo apt install nvidia-utils-535 For Code Profiling please see code_profiling.md .","title":"resource_mgmt"},{"location":"resource_management.html#resource-management-2-min-read","text":"\"Develop small, run big\" Resource usage decision tree JupyterHub | Teams calendar","title":"Resource management [2 min read]"},{"location":"resource_management.html#basic-rules-to-follow","text":"if you are using helpfull microservice, deploy it as separate pod and share the usage with team, e.g. Carla or SUCCESS6G services do EDA(Exploratory Data Analysis), develop, debug, and troubleshoot code on small JupyterHub pod or on your local machine run script on larger pod if absolutely necessary run the code on standalone server if ultra-super-duper-absolutely necessary develop solution using SLURM or SLURM on Kubernetes","title":"Basic rules to follow"},{"location":"resource_management.html#step-by-step","text":"Please follow these simple rules to avoid misuse of the SAI Research Unit computational resources: 1. spawn a small pod that is sufficient to process the data your are using * i.e. depending on the dataset size do a best guess and eventually go bigger 2. profile your code and pod resource usage 3. spawn a larger pod if needed 4. estimate a time and requirements and request a standalone server by putting a note into shared callendar in Teams - MANDATORY * make an ssh message - OPTIONAL (e.g. when you are chasing deadline and cant afford any mishaps) 5. [EXTREME] - develop a solution using multiple machines using SLURM, SLURM on Kubernetes, or other solution; and consult it with team","title":"Step-by-step"},{"location":"resource_management.html#gpu-inside-jupyterhub-pod","text":"To get the GPU running in a pod install a proper driver: sudo apt-get update sudo apt install nvidia-utils-535 For Code Profiling please see code_profiling.md .","title":"GPU inside JupyterHub pod"}]}